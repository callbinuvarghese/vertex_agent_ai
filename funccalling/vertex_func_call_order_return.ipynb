{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet google-cloud-aiplatform requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import vertexai\n",
    "from vertexai.generative_models import (\n",
    "    Content,\n",
    "    FunctionDeclaration,\n",
    "    GenerationConfig,\n",
    "    GenerativeModel,\n",
    "    Part,\n",
    "    Tool,\n",
    ")\n",
    "\n",
    "# Initialize Vertex AI\n",
    "vertexai.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Gemini model\n",
    "model = GenerativeModel(\"gemini-1.5-flash-001\",\n",
    "                        system_instruction=[\"\"\"You are a store support API assistant to help with online orders.\"\"\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the functions to be used by the model\n",
    "def get_order_status(order_id: str):\n",
    "    # Simulated response\n",
    "    return {\n",
    "        \"order_id\": order_id,\n",
    "        \"expected_delivery\": \"Tomorrow\"\n",
    "    }\n",
    "    \n",
    "get_order_status_func = FunctionDeclaration.from_func(get_order_status)\n",
    "\n",
    "initiate_return_func = FunctionDeclaration(\n",
    "    name=\"initiate_return\",\n",
    "    description=\"Initiate a return process for a given order ID.\",\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"order_id\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The unique identifier of the order to be returned.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"order_id\"]\n",
    "    },\n",
    ")\n",
    "\n",
    "# Define the tools that include the above functions\n",
    "support_tool = Tool(\n",
    "    function_declarations=[get_order_status_func, initiate_return_func],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the user's prompt in a Content object that we can reuse in model calls\n",
    "user_prompt_content = Content(\n",
    "    role=\"user\",\n",
    "    parts=[\n",
    "        #Part.from_text(\"What's the status of my order ID #12345?\"),\n",
    "        Part.from_text(\"I want to return my order with ID #12345?\")\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidates {\n",
      "  content {\n",
      "    role: \"model\"\n",
      "    parts {\n",
      "      function_call {\n",
      "        name: \"initiate_return\"\n",
      "        args {\n",
      "          fields {\n",
      "            key: \"order_id\"\n",
      "            value {\n",
      "              string_value: \"12345\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  finish_reason: STOP\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HATE_SPEECH\n",
      "    probability: NEGLIGIBLE\n",
      "    probability_score: 0.0888671875\n",
      "    severity: HARM_SEVERITY_NEGLIGIBLE\n",
      "    severity_score: 0.0717773438\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
      "    probability: NEGLIGIBLE\n",
      "    probability_score: 0.25390625\n",
      "    severity: HARM_SEVERITY_LOW\n",
      "    severity_score: 0.206054688\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HARASSMENT\n",
      "    probability: NEGLIGIBLE\n",
      "    probability_score: 0.108398438\n",
      "    severity: HARM_SEVERITY_NEGLIGIBLE\n",
      "    severity_score: 0.0654296875\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
      "    probability: NEGLIGIBLE\n",
      "    probability_score: 0.0549316406\n",
      "    severity: HARM_SEVERITY_NEGLIGIBLE\n",
      "    severity_score: 0.0255126953\n",
      "  }\n",
      "  avg_logprobs: -5.41866918204257e-08\n",
      "}\n",
      "usage_metadata {\n",
      "  prompt_token_count: 69\n",
      "  candidates_token_count: 11\n",
      "  total_token_count: 80\n",
      "}\n",
      "model_version: \"gemini-1.5-flash-001\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Send the prompt and instruct the model to generate content using the Tool that you just created\n",
    "response = model.generate_content(\n",
    "    user_prompt_content,\n",
    "    generation_config=GenerationConfig(temperature=0),\n",
    "    tools=[support_tool],\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"get_order_status\"\n",
      "args {\n",
      "  fields {\n",
      "    key: \"order_id\"\n",
      "    value {\n",
      "      string_value: \"12345\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "Your order #12345 is expected to be delivered tomorrow. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate over all the function calls in the model's response\n",
    "for function_call in response.candidates[0].function_calls:\n",
    "    print(function_call)\n",
    "\n",
    "    # Prepare a dummy response based on the function name\n",
    "    if function_call.name == \"get_order_status\":\n",
    "        # Extract the arguments to simulate the data\n",
    "        order_id = function_call.args[\"order_id\"]\n",
    "\n",
    "        # Dummy data for order status\n",
    "        api_response = {\n",
    "            \"order_id\": order_id,\n",
    "            \"expected_delivery\": \"Tomorrow\"\n",
    "        }\n",
    "\n",
    "    elif function_call.name == \"initiate_return\":\n",
    "        # Extract the arguments to simulate the data\n",
    "        order_id = function_call.args[\"order_id\"]\n",
    "        reason = function_call.args.get(\"reason\", \"No reason provided\")\n",
    "\n",
    "        # Dummy data for initiating a return\n",
    "        api_response = {\n",
    "            \"order_id\": order_id,\n",
    "            \"return_status\": \"Return initiated successfully.\",\n",
    "            \"return_label\": \"You will receive a return label shortly.\"\n",
    "        }\n",
    "\n",
    "    # Return the dummy API response to Gemini so it can generate a model response or request another function call\n",
    "    response = model.generate_content(\n",
    "        [\n",
    "            user_prompt_content,  # User prompt\n",
    "            response.candidates[0].content,  # Function call response\n",
    "            Content(\n",
    "                parts=[\n",
    "                    Part.from_function_response(\n",
    "                        name=function_call.name,\n",
    "                        response={\"content\": api_response},  # Return the dummy API response to Gemini\n",
    "                    ),\n",
    "                ],\n",
    "            ),\n",
    "        ],\n",
    "        tools=[support_tool],\n",
    "    )\n",
    "\n",
    "    # Get the model response and print it\n",
    "    print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
