{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet google-cloud-aiplatform requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --upgrade --quiet google-cloud-aiplatform requests\n",
    "import vertexai\n",
    "\n",
    "#vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "vertexai.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from vertexai.generative_models import (\n",
    "    FunctionDeclaration,\n",
    "    GenerationConfig,\n",
    "    GenerativeModel,\n",
    "    Part,\n",
    "    Tool,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stock_price = FunctionDeclaration(\n",
    "    name=\"get_stock_price\",\n",
    "    description=\"Fetch the current stock price of a given company\",\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"ticker\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Stock ticker symbol for a company\",\n",
    "            }\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "get_company_overview = FunctionDeclaration(\n",
    "    name=\"get_company_overview\",\n",
    "    description=\"Get company details and other financial data\",\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"ticker\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Stock ticker symbol for a company\",\n",
    "            }\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "get_company_news = FunctionDeclaration(\n",
    "    name=\"get_company_news\",\n",
    "    description=\"Get the latest news headlines for a given company.\",\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"tickers\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Stock ticker symbol for a company\",\n",
    "            }\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "get_news_with_sentiment = FunctionDeclaration(\n",
    "    name=\"get_news_with_sentiment\",\n",
    "    description=\"Gets live and historical market news and sentiment data\",\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"news_topic\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"\"\"News topic to learn about. Supported topics\n",
    "                               include blockchain, earnings, ipo,\n",
    "                               mergers_and_acquisitions, financial_markets,\n",
    "                               economy_fiscal, economy_monetary, economy_macro,\n",
    "                               energy_transportation, finance, life_sciences,\n",
    "                               manufacturing, real_estate, retail_wholesale,\n",
    "                               and technology\"\"\",\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "company_insights_tool = Tool(\n",
    "    function_declarations=[\n",
    "        get_stock_price,\n",
    "        get_company_overview,\n",
    "        get_company_news,\n",
    "        get_news_with_sentiment,\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API key for company and financial information\n",
    "#https://www.alphavantage.co/support/#api-key\n",
    "API_KEY = os.getenv(\"API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_price_from_api(content):\n",
    "    url = f\"https://www.alphavantage.co/query?function=GLOBAL_QUOTE&symbol={content['ticker']}&apikey={API_KEY}\"\n",
    "    api_request = requests.get(url)\n",
    "    return api_request.text\n",
    "\n",
    "\n",
    "def get_company_overview_from_api(content):\n",
    "    url = f\"https://www.alphavantage.co/query?function=OVERVIEW&symbol={content['ticker']}&apikey={API_KEY}\"\n",
    "    api_response = requests.get(url)\n",
    "    return api_response.text\n",
    "\n",
    "\n",
    "def get_company_news_from_api(content):\n",
    "    url = f\"https://www.alphavantage.co/query?function=NEWS_SENTIMENT&tickers={content['tickers']}&limit=20&sort=RELEVANCE&apikey={API_KEY}\"\n",
    "    api_response = requests.get(url)\n",
    "    return api_response.text\n",
    "\n",
    "\n",
    "def get_news_with_sentiment_from_api(content):\n",
    "    url = f\"https://www.alphavantage.co/query?function=NEWS_SENTIMENT&topics={content['news_topic']}&limit=20&sort=RELEVANCE&apikey={API_KEY}\"\n",
    "    api_request = requests.get(url)\n",
    "    return api_request.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_handler = {\n",
    "    \"get_stock_price\": get_stock_price_from_api,\n",
    "    \"get_company_overview\": get_company_overview_from_api,\n",
    "    \"get_company_news\": get_company_news_from_api,\n",
    "    \"get_news_with_sentiment\": get_news_with_sentiment_from_api,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_model = GenerativeModel(\n",
    "    \"gemini-1.5-pro-002\",\n",
    "    generation_config=GenerationConfig(temperature=0),\n",
    "    tools=[company_insights_tool],\n",
    ")\n",
    "\n",
    "\n",
    "chat = gemini_model.start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_chat_message(prompt):\n",
    "    display(Markdown(\"#### Prompt\"))\n",
    "    print(prompt, \"\\n\")\n",
    "    prompt += \"\"\"\n",
    "    Give a concise, high-level summary. Only use information that you learn from \n",
    "    the API responses. \n",
    "    \"\"\"\n",
    "\n",
    "    # Send a chat message to the Gemini API\n",
    "    response = chat.send_message(prompt)\n",
    "\n",
    "    # Handle cases with multiple chained function calls\n",
    "    function_calling_in_process = True\n",
    "    while function_calling_in_process:\n",
    "        # Extract the function call response\n",
    "        print(response)\n",
    "        function_call = response.candidates[0].content.parts[0].function_call\n",
    "        if not function_call:\n",
    "            break\n",
    "\n",
    "        print(function_call)\n",
    "\n",
    "        # Check for a function call or a natural language response\n",
    "        if function_call.name in function_handler.keys():\n",
    "            # Extract the function call name\n",
    "            function_name = function_call.name\n",
    "            display(Markdown(\"#### Predicted function name\"))\n",
    "            print(function_name, \"\\n\")\n",
    "\n",
    "            # Extract the function call parameters\n",
    "            params = {key: value for key, value in function_call.args.items()}\n",
    "            display(Markdown(\"#### Predicted function parameters\"))\n",
    "            print(params, \"\\n\")\n",
    "\n",
    "            # Invoke a function that calls an external API\n",
    "            function_api_response = function_handler[function_name](params)[\n",
    "                :20000\n",
    "            ]  # Stay within the input token limit\n",
    "            display(Markdown(\"#### API response\"))\n",
    "            print(function_api_response[:500], \"...\", \"\\n\")\n",
    "            print(f\"function_name:{function_name}\")\n",
    "\n",
    "            # Send the API response back to Gemini, which will generate a natural language summary or another function call\n",
    "            response = chat.send_message(\n",
    "                Part.from_function_response(\n",
    "                    name=function_name,\n",
    "                    response={\"content\": function_api_response},\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            function_calling_in_process = False\n",
    "\n",
    "    # Show the final natural language summary\n",
    "    display(Markdown(\"#### Natural language response\"))\n",
    "    display(Markdown(response.text.replace(\"$\", \"\\\\$\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Prompt"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me a company overview of Walmart and The Home Depot \n",
      "\n",
      "candidates {\n",
      "  content {\n",
      "    role: \"model\"\n",
      "    parts {\n",
      "      function_call {\n",
      "        name: \"get_company_overview\"\n",
      "        args {\n",
      "          fields {\n",
      "            key: \"ticker\"\n",
      "            value {\n",
      "              string_value: \"WMT\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    parts {\n",
      "      function_call {\n",
      "        name: \"get_company_overview\"\n",
      "        args {\n",
      "          fields {\n",
      "            key: \"ticker\"\n",
      "            value {\n",
      "              string_value: \"HD\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  finish_reason: STOP\n",
      "  avg_logprobs: -7.9482560977339751e-09\n",
      "}\n",
      "usage_metadata {\n",
      "  prompt_token_count: 195\n",
      "  candidates_token_count: 15\n",
      "  total_token_count: 210\n",
      "}\n",
      "model_version: \"gemini-1.5-pro-002\"\n",
      "\n",
      "name: \"get_company_overview\"\n",
      "args {\n",
      "  fields {\n",
      "    key: \"ticker\"\n",
      "    value {\n",
      "      string_value: \"WMT\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Predicted function name"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_company_overview \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Predicted function parameters"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ticker': 'WMT'} \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### API response"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Symbol\": \"WMT\",\n",
      "    \"AssetType\": \"Common Stock\",\n",
      "    \"Name\": \"Walmart Inc\",\n",
      "    \"Description\": \"Walmart Inc. is an American multinational retail corporation that operates a chain of hypermarkets, discount department stores, and grocery stores from the United States, headquartered in Bentonville, Arkansas. It also owns and operates Sam's Club retail warehouses.\",\n",
      "    \"CIK\": \"104169\",\n",
      "    \"Exchange\": \"NYSE\",\n",
      "    \"Currency\": \"USD\",\n",
      "    \"Country\": \"USA\",\n",
      "    \"Sector\": \"TRADE & SERVICES\",\n",
      "     ... \n",
      "\n",
      "function_name:get_company_overview\n"
     ]
    },
    {
     "ename": "BadRequest",
     "evalue": "400 POST https://us-central1-aiplatform.googleapis.com/v1/projects/ai-project-444521/locations/us-central1/publishers/google/models/gemini-1.5-pro-002:generateContent?%24alt=json%3Benum-encoding%3Dint: Please ensure that the number of function response parts should be equal to number of function call parts of the function call turn.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequest\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#send_chat_message(\"What is the current stock price for Google?\")\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#send_chat_message(\"Give me a company overview of Google\")\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#send_chat_message(\"What are the latest news headlines for Google?\")\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43msend_chat_message\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGive me a company overview of Walmart and The Home Depot\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#send_chat_message(\"Has there been any exciting news related to real estate recently?\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 44\u001b[0m, in \u001b[0;36msend_chat_message\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_name:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# Send the API response back to Gemini, which will generate a natural language summary or another function call\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_message\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mPart\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_function_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_api_response\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     function_calling_in_process \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/vertexai/generative_models/_generative_models.py:1222\u001b[0m, in \u001b[0;36mChatSession.send_message\u001b[0;34m(self, content, generation_config, safety_settings, tools, labels, stream)\u001b[0m\n\u001b[1;32m   1214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_message_streaming(\n\u001b[1;32m   1215\u001b[0m         content\u001b[38;5;241m=\u001b[39mcontent,\n\u001b[1;32m   1216\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1219\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m   1220\u001b[0m     )\n\u001b[1;32m   1221\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_message\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafety_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafety_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/vertexai/generative_models/_generative_models.py:1351\u001b[0m, in \u001b[0;36mChatSession._send_message\u001b[0;34m(self, content, generation_config, safety_settings, tools, labels)\u001b[0m\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1350\u001b[0m     request_history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_history \u001b[38;5;241m+\u001b[39m history_delta\n\u001b[0;32m-> 1351\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1354\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafety_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafety_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1357\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1358\u001b[0m     \u001b[38;5;66;03m# By default we're not adding incomplete interactions to history.\u001b[39;00m\n\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_validator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/vertexai/generative_models/_generative_models.py:744\u001b[0m, in \u001b[0;36m_GenerativeModel._generate_content\u001b[0;34m(self, contents, generation_config, safety_settings, tools, tool_config, labels)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generates content.\u001b[39;00m\n\u001b[1;32m    718\u001b[0m \n\u001b[1;32m    719\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;124;03m    A single GenerationResponse object\u001b[39;00m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    736\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_request(\n\u001b[1;32m    737\u001b[0m     contents\u001b[38;5;241m=\u001b[39mcontents,\n\u001b[1;32m    738\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    742\u001b[0m     labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m    743\u001b[0m )\n\u001b[0;32m--> 744\u001b[0m gapic_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prediction_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_response(gapic_response)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/google/cloud/aiplatform_v1/services/prediction_service/client.py:2208\u001b[0m, in \u001b[0;36mPredictionServiceClient.generate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   2205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m   2207\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m-> 2208\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2213\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m   2216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/google/api_core/grpc_helpers.py:76\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(callable_)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merror_remapped_callable\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/google/cloud/aiplatform_v1/services/prediction_service/transports/rest.py:1304\u001b[0m, in \u001b[0;36mPredictionServiceRestTransport._GenerateContent.__call__\u001b[0;34m(self, request, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;66;03m# In case of error, raise the appropriate core_exceptions.GoogleAPICallError exception\u001b[39;00m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;66;03m# subclass.\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m400\u001b[39m:\n\u001b[0;32m-> 1304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core_exceptions\u001b[38;5;241m.\u001b[39mfrom_http_response(response)\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;66;03m# Return the response\u001b[39;00m\n\u001b[1;32m   1307\u001b[0m resp \u001b[38;5;241m=\u001b[39m prediction_service\u001b[38;5;241m.\u001b[39mGenerateContentResponse()\n",
      "\u001b[0;31mBadRequest\u001b[0m: 400 POST https://us-central1-aiplatform.googleapis.com/v1/projects/ai-project-444521/locations/us-central1/publishers/google/models/gemini-1.5-pro-002:generateContent?%24alt=json%3Benum-encoding%3Dint: Please ensure that the number of function response parts should be equal to number of function call parts of the function call turn."
     ]
    }
   ],
   "source": [
    "#send_chat_message(\"What is the current stock price for Google?\")\n",
    "#send_chat_message(\"Give me a company overview of Google\")\n",
    "#send_chat_message(\"What are the latest news headlines for Google?\")\n",
    "send_chat_message(\"Give me a company overview of Walmart and The Home Depot\")\n",
    "#send_chat_message(\"Has there been any exciting news related to real estate recently?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
